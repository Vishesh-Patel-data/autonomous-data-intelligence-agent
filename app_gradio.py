"""
app_gradio.py

Gradio UI that calls your FastAPI backend:

- Tab 1: User enters a local tabular file path (CSV/Excel)
- Tab 2: User uploads CSV/Excel + optional PDFs
- Tab 3: User chats with an AI assistant about the dataset using the EDA report

Both tabs 1 & 2 call POST /run-pipeline/local on http://127.0.0.1:8000
and show:
- basic dataset stats
- EDA report (Markdown) generated by Gemini
- combined PDF summary (if any PDFs were provided)

Tab 3 uses the EDA report as context for a Gemini-powered chatbot.
"""

import os
import requests
import gradio as gr

# FastAPI backend location
BACKEND_URL = "http://127.0.0.1:8000"

# Gemini HTTP config (same style as in llm_utils)
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY")
GEMINI_MODEL = "gemini-2.5-flash-lite"
GEMINI_URL = (
    f"https://generativelanguage.googleapis.com/v1beta/models/"
    f"{GEMINI_MODEL}:generateContent"
)

# ------------------------------------------------------------
# Helpers to call backend
# ------------------------------------------------------------

def run_pipeline_from_path(path: str):
    """
    Call the FastAPI backend with a local tabular path only.
    PDFs are not passed here (used only in upload mode).
    """
    if not path:
        return "**Error: please enter a valid local file path.**", "", "", ""

    url = f"{BACKEND_URL}/run-pipeline/local"
    payload = {"tabular_path": path}

    try:
        resp = requests.post(url, json=payload, timeout=600)
    except Exception as e:
        msg = f"**Error:** Could not reach backend service: {e}"
        return msg, "", "", ""

    if resp.status_code != 200:
        msg = f"**Backend error {resp.status_code}:** {resp.text}"
        return msg, "", "", ""

    data = resp.json()

    # ----- Dataset summary -----
    raw_rows = data.get("raw_rows", 0)
    raw_cols = data.get("raw_cols", 0)
    cleaned_rows = data.get("cleaned_rows", 0)
    cleaned_cols = data.get("cleaned_cols", 0)
    columns = data.get("columns", [])

    meta_md = f"""
### üìä Dataset Summary

- **Raw shape:** {raw_rows} rows √ó {raw_cols} columns  
- **Cleaned shape:** {cleaned_rows} rows √ó {cleaned_cols} columns  
- **Columns:** `{', '.join(columns)}`  
"""

    # ----- EDA report -----
    eda_md = data.get("eda_report_markdown", "No EDA report returned from backend.")

    # ----- PDF combined summary (if any) -----
    pdf_md = data.get(
        "pdf_combined_summary_markdown",
        "No PDF combined summary for this run.",
    )

    # Store EDA markdown into state (for chatbot use later)
    eda_state_value = eda_md

    return meta_md, eda_md, pdf_md, eda_state_value


def run_pipeline_from_upload(uploaded_files):
    """
    Handle Gradio file upload with multiple files:
    - pick the first CSV/Excel as the tabular file
    - collect all PDFs as pdf_paths
    - send both to the backend
    """
    if not uploaded_files:
        msg = "**Error:** Please upload at least one CSV/Excel file."
        return msg, "", "", ""

    # Gradio with file_count='multiple' gives a list; be defensive anyway.
    if isinstance(uploaded_files, str):
        paths = [uploaded_files]
    elif isinstance(uploaded_files, list):
        paths = [str(p) for p in uploaded_files]
    else:
        paths = [str(uploaded_files)]

    tabular_exts = (".csv", ".xlsx", ".xls")
    tabular_path = None
    pdf_paths = []

    for p in paths:
        lower = p.lower()
        if lower.endswith(tabular_exts) and tabular_path is None:
            tabular_path = p
        elif lower.endswith(".pdf"):
            pdf_paths.append(p)

    if tabular_path is None:
        msg = (
            "**Error:** No CSV/Excel file found in upload. "
            "Right now the pipeline requires at least one tabular file. "
            "PDF-only uploads are not yet supported."
        )
        return msg, "", "", ""

    url = f"{BACKEND_URL}/run-pipeline/local"
    payload = {
        "tabular_path": tabular_path,
        "pdf_paths": pdf_paths,
    }

    try:
        resp = requests.post(url, json=payload, timeout=600)
    except Exception as e:
        msg = f"**Error:** Could not reach backend service: {e}"
        return msg, "", "", ""

    if resp.status_code != 200:
        msg = f"**Backend error {resp.status_code}:** {resp.text}"
        return msg, "", "", ""

    data = resp.json()

    # ----- Dataset summary -----
    raw_rows = data.get("raw_rows", 0)
    raw_cols = data.get("raw_cols", 0)
    cleaned_rows = data.get("cleaned_rows", 0)
    cleaned_cols = data.get("cleaned_cols", 0)
    columns = data.get("columns", [])

    meta_md = f"""
### üìä Dataset Summary

- **Raw shape:** {raw_rows} rows √ó {raw_cols} columns  
- **Cleaned shape:** {cleaned_rows} rows √ó {cleaned_cols} columns  
- **Columns:** `{', '.join(columns)}`  
"""

    # ----- EDA report -----
    eda_md = data.get("eda_report_markdown", "No EDA report returned from backend.")

    # ----- PDF combined summary (if any) -----
    pdf_md = data.get(
        "pdf_combined_summary_markdown",
        "No PDF combined summary for this run.",
    )

    eda_state_value = eda_md

    return meta_md, eda_md, pdf_md, eda_state_value


# ------------------------------------------------------------
# Helper to call Gemini HTTP API for chat
# ------------------------------------------------------------

def call_gemini_with_context(system_prompt: str) -> str:
    """
    Low-level HTTP call to Gemini 2.5 Flash Lite using the given prompt.
    Returns the response text, or an error message.
    """
    if not GEMINI_API_KEY:
        return "Error: GOOGLE_API_KEY environment variable is not set."

    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": GEMINI_API_KEY,
    }

    body = {
        "contents": [
            {
                "parts": [
                    {"text": system_prompt}
                ]
            }
        ]
    }

    try:
        resp = requests.post(GEMINI_URL, headers=headers, json=body, timeout=300)
        resp.raise_for_status()
        data = resp.json()

        candidates = data.get("candidates", [])
        if not candidates:
            return "LLM returned no candidates."

        content = candidates[0].get("content", {})
        parts = content.get("parts", [])
        if not parts:
            return "LLM returned no content parts."

        text = parts[0].get("text", "")
        return text or "LLM returned empty text."

    except requests.exceptions.HTTPError as http_err:
        return f"LLM HTTP error: {http_err}\nResponse: {resp.text}"
    except Exception as e:
        return f"LLM call failed: {e}"


# ------------------------------------------------------------
# Chat handler
# ------------------------------------------------------------

def chat_with_data(user_message, history, eda_context):
    """
    Chat handler for the 'Chat with Your Data' tab.

    - history: list of [user, bot] pairs (managed by Gradio)
    - eda_context: EDA report markdown stored in gr.State

    We build a prompt that includes:
    - short instruction
    - EDA report
    - conversation history
    - latest user question
    """
    if not user_message:
        return history

    if not eda_context:
        bot_reply = (
            "I don't have any dataset context yet. "
            "Please run the pipeline first (via the Path or Upload tab), "
            "then come back here to ask questions."
        )
        history = history + [[user_message, bot_reply]]
        return history

    # Build conversation string
    conv_lines = []
    for u, b in history:
        conv_lines.append(f"User: {u}")
        conv_lines.append(f"Assistant: {b}")
    conv_text = "\n".join(conv_lines)

    prompt = f"""
You are a helpful data analyst assistant.

You are given this EDA report for a dataset (in Markdown):

---
{eda_context}
---

Conversation so far:
{conv_text}

The user has asked a new question:

User: {user_message}

Instructions:
- Answer using the EDA report and reasonable general reasoning.
- If something is NOT supported by the EDA context, say so explicitly.
- Use clear, structured Markdown.
- Be concise but insightful.
"""

    llm_answer = call_gemini_with_context(prompt)
    history = history + [[user_message, llm_answer]]
    return history


# ------------------------------------------------------------
# Gradio UI definition
# ------------------------------------------------------------

with gr.Blocks(title="Autonomous Data Intelligence ‚Äì Pipeline + Chat") as demo:
    gr.Markdown("# üß† Autonomous Data Intelligence")
    gr.Markdown(
        "This app:\n"
        "1. Calls your FastAPI backend to run cleaning + EDA.\n"
        "2. Uses Gemini to generate an EDA report.\n"
        "3. Lets you chat with an AI assistant about the dataset.\n\n"
        "> **Make sure your backend is running:**\n"
        "> ```bash\n"
        "> uvicorn src.service:app --reload\n"
        "> ```"
    )

    # Hidden state: stores the latest EDA report for chat
    eda_state = gr.State("")

    with gr.Tab("1Ô∏è‚É£ Use Local File Path"):
        gr.Markdown(
            "Enter the full path to a local CSV/Excel file (on the **backend machine**), "
            "then click **Run Pipeline**."
        )

        path_input = gr.Textbox(
            label="Local tabular file path (CSV/Excel)",
            placeholder="C:\\Users\\91727\\Downloads\\TUVYproperty_details.xlsx",
            lines=1,
        )

        run_button_path = gr.Button("üöÄ Run Pipeline (Path)")

    with gr.Tab("2Ô∏è‚É£ Upload File(s)"):
        gr.Markdown(
            "Upload a CSV or Excel file from your computer, plus optional PDFs. "
            "The app will run the pipeline on the tabular file and summarize PDFs."
        )

        upload_input = gr.File(
            label="Upload CSV/Excel and/or PDFs",
            file_types=[".csv", ".xlsx", ".xls", ".pdf"],
            type="filepath",
            file_count="multiple",  # allow multiple files at once
        )

        run_button_upload = gr.Button("üöÄ Run Pipeline (Upload)")

    with gr.Tab("3Ô∏è‚É£ Chat with Your Data"):
        gr.Markdown(
            "After you run the pipeline (via **Tab 1 or 2**), you can ask questions "
            "about the dataset here. The assistant will use the EDA report as context."
        )

        chatbot = gr.Chatbot(label="Data Assistant")
        chat_input = gr.Textbox(
            label="Your question",
            placeholder="e.g., What are the most common sale prices?",
            lines=1,
        )
        send_btn = gr.Button("Send")

    # Shared outputs for tabs 1 & 2 (shown below tabs)
    meta_output = gr.Markdown(label="Dataset Summary")
    eda_output = gr.Markdown(label="EDA Report (Gemini)")
    pdf_output = gr.Markdown(label="PDF Combined Summary (if any)")

    # Wire buttons ‚Üí functions ‚Üí outputs (including updating eda_state)
    run_button_path.click(
        fn=run_pipeline_from_path,
        inputs=[path_input],
        outputs=[meta_output, eda_output, pdf_output, eda_state],
    )

    run_button_upload.click(
        fn=run_pipeline_from_upload,
        inputs=[upload_input],
        outputs=[meta_output, eda_output, pdf_output, eda_state],
    )

    # Chat interactions: use eda_state as context
    send_btn.click(
        fn=chat_with_data,
        inputs=[chat_input, chatbot, eda_state],
        outputs=[chatbot],
    )
    # Optional: clear the input box after sending
    send_btn.click(
        fn=lambda: "",
        inputs=None,
        outputs=[chat_input],
    )

    # Also allow pressing Enter in the textbox to send
    chat_input.submit(
        fn=chat_with_data,
        inputs=[chat_input, chatbot, eda_state],
        outputs=[chatbot],
    ).then(
        fn=lambda: "",
        inputs=None,
        outputs=[chat_input],
    )


if __name__ == "__main__":
    demo.launch()
