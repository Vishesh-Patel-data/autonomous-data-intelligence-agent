"""
app_gradio.py

Gradio UI that calls your FastAPI backend:

- Tab 1: User enters a local tabular file path (CSV/Excel)
- Tab 2: User uploads a CSV/Excel file via the browser
- Tab 3: User chats with an AI assistant about the dataset using the EDA report

Both tabs 1 & 2 call POST /run-pipeline/local on http://127.0.0.1:8000
and show:
- basic dataset stats
- EDA report (Markdown) generated by Gemini

Tab 3 uses the EDA report as context for a Gemini-powered chatbot.
"""

import os
import requests
import gradio as gr

# FastAPI backend location
BACKEND_URL = "http://127.0.0.1:8000"

# Gemini HTTP config (same style as in llm_utils)
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY")
GEMINI_MODEL = "gemini-2.5-flash-lite"
GEMINI_URL = (
    f"https://generativelanguage.googleapis.com/v1beta/models/"
    f"{GEMINI_MODEL}:generateContent"
)


# ------------------------------------------------------------
# Helpers to call backend
# ------------------------------------------------------------

def run_pipeline_from_path(tabular_path: str):
    """
    Call the FastAPI /run-pipeline/local endpoint with a local path.
    Returns:
      - meta_info (Markdown)
      - eda_report_markdown (Markdown)
      - eda_report_markdown again (for state storage)
    """
    tabular_path = (tabular_path or "").strip()

    if not tabular_path:
        msg = "**Error:** Please provide a local CSV/Excel file path."
        return msg, "", ""

    try:
        payload = {"tabular_path": tabular_path}
        resp = requests.post(
            f"{BACKEND_URL}/run-pipeline/local",
            json=payload,
            timeout=300,
        )

        if resp.status_code != 200:
            # Show backend error body for debugging
            msg = (
                f"**Backend error {resp.status_code}:**\n\n"
                f"```json\n{resp.text}\n```"
            )
            return msg, "", ""

        data = resp.json()

        meta_md = (
            f"### üìä Dataset Summary\n"
            f"- **Raw shape:** {data['raw_rows']} rows √ó {data['raw_cols']} columns\n"
            f"- **Cleaned shape:** {data['cleaned_rows']} rows √ó {data['cleaned_cols']} columns\n"
            f"- **Columns:** {', '.join(data['columns'])}\n"
        )

        eda_md = data.get("eda_report_markdown", "")
        if not eda_md:
            eda_md = "_No EDA report returned from backend._"

        # Third return value goes into a hidden gr.State so chat can use it
        return meta_md, eda_md, eda_md

    except Exception as e:
        msg = f"**Request failed:** `{e}`"
        return msg, "", ""


def run_pipeline_from_upload(uploaded_file):
    """
    Handle Gradio file upload:
    - Gradio gives us a filepath (because type='filepath')
    - We simply pass that path to the same backend endpoint
    """
    if uploaded_file is None:
        msg = "**Error:** Please upload a CSV/Excel file first."
        return msg, "", ""

    # When type='filepath', uploaded_file is a str path
    file_path = str(uploaded_file)
    if not file_path:
        return "**Error:** Could not determine file path from upload.", "", ""

    return run_pipeline_from_path(file_path)


# ------------------------------------------------------------
# Helper to call Gemini HTTP API for chat
# ------------------------------------------------------------

def call_gemini_with_context(system_prompt: str) -> str:
    """
    Low-level HTTP call to Gemini 2.5 Flash Lite using the given prompt.
    Returns the response text, or an error message.
    """
    if not GEMINI_API_KEY:
        return "Error: GOOGLE_API_KEY environment variable is not set."

    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": GEMINI_API_KEY,
    }

    body = {
        "contents": [
            {
                "parts": [
                    {"text": system_prompt}
                ]
            }
        ]
    }

    try:
        resp = requests.post(GEMINI_URL, headers=headers, json=body, timeout=300)
        resp.raise_for_status()
        data = resp.json()

        candidates = data.get("candidates", [])
        if not candidates:
            return "LLM returned no candidates."

        content = candidates[0].get("content", {})
        parts = content.get("parts", [])
        if not parts:
            return "LLM returned no content parts."

        text = parts[0].get("text", "")
        return text or "LLM returned empty text."

    except requests.exceptions.HTTPError as http_err:
        return f"LLM HTTP error: {http_err}\nResponse: {resp.text}"
    except Exception as e:
        return f"LLM call failed: {e}"


# ------------------------------------------------------------
# Chat handler
# ------------------------------------------------------------

def chat_with_data(user_message, history, eda_context):
    """
    Chat handler for the 'Chat with Your Data' tab.

    - history: list of [user, bot] pairs (managed by Gradio)
    - eda_context: EDA report markdown stored in gr.State

    We build a prompt that includes:
    - short instruction
    - EDA report
    - conversation history
    - latest user question
    """
    if not user_message:
        return history

    if not eda_context:
        bot_reply = (
            "I don't have any dataset context yet. "
            "Please run the pipeline first (via the Path or Upload tab), "
            "then come back here to ask questions."
        )
        history = history + [[user_message, bot_reply]]
        return history

    # Build conversation string
    conv_lines = []
    for u, b in history:
        conv_lines.append(f"User: {u}")
        conv_lines.append(f"Assistant: {b}")
    conv_text = "\n".join(conv_lines)

    prompt = f"""
You are a helpful data analyst assistant.

You are given this EDA report for a dataset (in Markdown):

---
{eda_context}
---

Conversation so far:
{conv_text}

The user has asked a new question:

User: {user_message}

Instructions:
- Answer using the EDA report and reasonable general reasoning.
- If something is NOT supported by the EDA context, say so explicitly.
- Use clear, structured Markdown.
- Be concise but insightful.
"""

    llm_answer = call_gemini_with_context(prompt)
    history = history + [[user_message, llm_answer]]
    return history


# ------------------------------------------------------------
# Gradio UI definition
# ------------------------------------------------------------

with gr.Blocks(title="Autonomous Data Intelligence ‚Äì Pipeline + Chat") as demo:
    gr.Markdown("# üß† Autonomous Data Intelligence")
    gr.Markdown(
        "This app:\n"
        "1. Calls your FastAPI backend to run cleaning + EDA.\n"
        "2. Uses Gemini to generate an EDA report.\n"
        "3. Lets you chat with an AI assistant about the dataset.\n\n"
        "> **Make sure your backend is running:**\n"
        "> ```bash\n"
        "> uvicorn src.service:app --reload\n"
        "> ```"
    )

    # Hidden state: stores the latest EDA report for chat
    eda_state = gr.State("")

    with gr.Tab("1Ô∏è‚É£ Use Local File Path"):
        gr.Markdown(
            "Enter the full path to a local CSV/Excel file (on the **backend machine**), "
            "then click **Run Pipeline**."
        )

        tabular_path_input = gr.Textbox(
            label="Local tabular file path (CSV/Excel)",
            placeholder="C:\\Users\\91727\\Downloads\\TUVYproperty_details.xlsx",
            lines=1,
        )

        run_button_path = gr.Button("üöÄ Run Pipeline (Path)")

    with gr.Tab("2Ô∏è‚É£ Upload File"):
        gr.Markdown(
            "Upload a CSV or Excel file from your computer. "
            "The app will use a temporary path and run the pipeline on it."
        )

        upload_input = gr.File(
            label="Upload CSV/Excel",
            file_types=[".csv", ".xlsx", ".xls"],
            type="filepath",
        )

        run_button_upload = gr.Button("üöÄ Run Pipeline (Upload)")

    with gr.Tab("3Ô∏è‚É£ Chat with Your Data"):
        gr.Markdown(
            "After you run the pipeline (via **Tab 1 or 2**), you can ask questions "
            "about the dataset here. The assistant will use the EDA report as context."
        )

        chatbot = gr.Chatbot(label="Data Assistant")
        chat_input = gr.Textbox(
            label="Your question",
            placeholder="e.g., What are the most common sale prices?",
            lines=1,
        )
        send_btn = gr.Button("Send")

    # Shared outputs for tabs 1 & 2
    meta_output = gr.Markdown(label="Dataset Summary")
    eda_output = gr.Markdown(label="EDA Report (Gemini)")

    # Wire buttons ‚Üí functions ‚Üí outputs (including updating eda_state)
    run_button_path.click(
        fn=run_pipeline_from_path,
        inputs=[tabular_path_input],
        outputs=[meta_output, eda_output, eda_state],
    )

    run_button_upload.click(
        fn=run_pipeline_from_upload,
        inputs=[upload_input],
        outputs=[meta_output, eda_output, eda_state],
    )

    # Chat interactions: use eda_state as context
    send_btn.click(
        fn=chat_with_data,
        inputs=[chat_input, chatbot, eda_state],
        outputs=[chatbot],
    )
    # Optional: clear the input box after sending
    send_btn.click(
        fn=lambda: "",
        inputs=None,
        outputs=[chat_input],
    )

    # Also allow pressing Enter in the textbox to send
    chat_input.submit(
        fn=chat_with_data,
        inputs=[chat_input, chatbot, eda_state],
        outputs=[chatbot],
    ).then(
        fn=lambda: "",
        inputs=None,
        outputs=[chat_input],
    )


if __name__ == "__main__":
    demo.launch()
